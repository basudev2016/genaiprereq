{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['amazing' 'in' 'is' 'language' 'learning' 'love' 'natural' 'new' 'nlp'\n",
      " 'processing' 'things']\n",
      "\n",
      "Count Vectors:\n",
      " [[0 0 0 1 0 1 1 0 0 1 0]\n",
      " [1 0 1 1 0 0 0 0 0 1 0]\n",
      " [0 1 0 0 1 1 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus (list of text documents)\n",
    "corpus = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Language processing is amazing\",\n",
    "    \"I love learning new things in NLP\"\n",
    "]\n",
    "\n",
    "# Initialize the Count Vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the model and transform the text data to count vectors\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the result to an array for easier visualization\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Get feature names (unique words in the corpus)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the results\n",
    "print(\"Feature Names:\", feature_names)\n",
    "print(\"\\nCount Vectors:\\n\", X_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['amazing' 'in' 'is' 'language' 'learning' 'love' 'natural' 'new' 'nlp'\n",
      " 'processing' 'things']\n",
      "\n",
      "TF-IDF Vectors:\n",
      " [[0.         0.         0.         0.45985353 0.         0.45985353\n",
      "  0.60465213 0.         0.         0.45985353 0.        ]\n",
      " [0.5628291  0.         0.5628291  0.42804604 0.         0.\n",
      "  0.         0.         0.         0.42804604 0.        ]\n",
      " [0.         0.42339448 0.         0.         0.42339448 0.32200242\n",
      "  0.         0.42339448 0.42339448 0.         0.42339448]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample corpus (list of text documents)\n",
    "corpus = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Language processing is amazing\",\n",
    "    \"I love learning new things in NLP\"\n",
    "]\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the model and transform the text data to TF-IDF vectors\n",
    "X = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the result to an array for easier visualization\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Get feature names (unique words in the corpus)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the results\n",
    "print(\"Feature Names:\", feature_names)\n",
    "print(\"\\nTF-IDF Vectors:\\n\", X_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec / GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'language':\n",
      " [-0.01723938  0.00733148  0.01037977  0.01148388  0.01493384 -0.01233535\n",
      "  0.00221123  0.01209456 -0.0056801  -0.01234705 -0.00082045 -0.0167379\n",
      " -0.01120002  0.01420908  0.00670508  0.01445134  0.01360049  0.01506148\n",
      " -0.00757831 -0.00112361  0.00469675 -0.00903806  0.01677746 -0.01971633\n",
      "  0.01352928  0.00582883 -0.00986566  0.00879638 -0.00347915  0.01342277\n",
      "  0.0199297  -0.00872489 -0.00119868 -0.01139127  0.00770164  0.00557325\n",
      "  0.01378215  0.01220219  0.01907699  0.01854683  0.01579614 -0.01397901\n",
      " -0.01831173 -0.00071151 -0.00619968  0.01578863  0.01187715 -0.00309133\n",
      "  0.00302193  0.00358008]\n",
      "\n",
      "Most similar words to 'language':\n",
      "I: 0.17\n",
      "learning: 0.16\n",
      "in: 0.14\n",
      "processing: 0.13\n",
      "new: 0.12\n",
      "Natural: 0.09\n",
      "natural: 0.03\n",
      "Language: 0.02\n",
      "is: 0.01\n",
      "fun: -0.03\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    [\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"Language\", \"processing\", \"is\", \"amazing\"],\n",
    "    [\"I\", \"love\", \"learning\", \"new\", \"things\", \"in\", \"NLP\"],\n",
    "    [\"Natural\", \"language\", \"processing\", \"is\", \"fun\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model with CBOW\n",
    "model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, sg=0)  # sg=0 for CBOW, sg=1 for Skip-gram\n",
    "\n",
    "# Get the vector for a word\n",
    "print(\"Vector for 'language':\\n\", model.wv['language'])\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar(\"language\")\n",
    "print(\"\\nMost similar words to 'language':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the embeddings from Glove official site https://nlp.stanford.edu/projects/glove/\n",
    "unzip the 6B file and provide the path here bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16320\\3247570286.py:7: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, word2vec_output_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'language':\n",
      " [ 0.18519   0.34111   0.36097   0.27093  -0.031335  0.83923  -0.50534\n",
      " -0.80062   0.40695   0.82488  -0.98239  -0.6354   -0.21382   0.079889\n",
      " -0.29557   0.17075   0.17479  -0.74214  -0.2677    0.21074  -0.41795\n",
      "  0.027713  0.71123   0.2063   -0.12266  -0.80088   0.22942   0.041037\n",
      " -0.56901   0.097472 -0.59139   1.0524   -0.66803  -0.70471   0.69757\n",
      " -0.11137  -0.27816   0.047361  0.020305 -0.184    -1.0254    0.11297\n",
      " -0.79547   0.41642  -0.2508   -0.3188    0.37044  -0.26873  -0.36185\n",
      " -0.096621 -0.029956  0.67308   0.53102   0.62816  -0.11507  -1.5524\n",
      " -0.30628  -0.4253    1.8887    0.3247    0.60202   0.81163  -0.46029\n",
      " -1.4061    0.80229   0.2019    0.60938   0.063545  0.21925  -0.043372\n",
      " -0.36648   0.61308   1.0207   -0.39014   0.1717    0.61272  -0.80342\n",
      "  0.71295  -1.0938   -0.50546  -0.99668  -1.6701   -0.31804  -0.62934\n",
      " -2.0226    0.79405  -0.16994  -0.37627   0.57998   0.16643   0.1356\n",
      "  0.0943   -0.24154   0.7123   -0.4201    0.24735  -0.94449  -1.0794\n",
      "  0.3413    0.34704 ]\n",
      "\n",
      "Most similar words to 'language':\n",
      "languages: 0.83\n",
      "word: 0.75\n",
      "spoken: 0.74\n",
      "arabic: 0.73\n",
      "english: 0.72\n",
      "dialect: 0.69\n",
      "vocabulary: 0.69\n",
      "text: 0.69\n",
      "translation: 0.68\n",
      "words: 0.67\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Convert GloVe file format to Word2Vec format (run this once)\n",
    "glove_file = 'D:/Basudev/genaiprereq/Day2/glove.6B/glove.6B.100d.txt'  # Update with your path\n",
    "word2vec_output_file = 'glove.6B.100d.word2vec.txt'\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_file, word2vec_output_file)\n",
    "\n",
    "# Load the converted model\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "\n",
    "# Check the vector for a word\n",
    "print(\"Vector for 'language':\\n\", model['language'])\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.most_similar(\"language\")\n",
    "print(\"\\nMost similar words to 'language':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'language':\n",
      " [-4.6425052e-03  3.6429535e-03  1.2540421e-03 -3.6066731e-03\n",
      " -3.2773227e-03  2.4448987e-03 -6.6240068e-04  5.1185279e-04\n",
      " -2.1971019e-03 -1.1697644e-03  1.7850193e-03 -1.0030718e-03\n",
      "  3.3278466e-05  1.0310608e-03  3.8224044e-03  1.8119046e-03\n",
      "  1.2733500e-03  5.3642802e-03  2.2080135e-04 -1.1716852e-03\n",
      " -2.3896585e-04 -9.9625357e-04 -1.1096887e-03  3.0506297e-03\n",
      "  5.1598290e-05 -3.7843618e-04 -5.6672175e-03 -2.2503231e-03\n",
      "  3.3958235e-03 -2.9582152e-04  4.9308175e-03  5.3104747e-04\n",
      "  5.3159345e-04 -3.1725504e-06 -1.5835118e-03 -1.6003581e-03\n",
      "  1.8443600e-03  9.1458380e-04 -2.5605112e-03  3.3275173e-03\n",
      " -7.8906858e-04  2.9359653e-03 -6.4219598e-04  1.5274928e-03\n",
      "  1.5035694e-03 -1.5052463e-04  2.5598896e-03 -1.7102089e-03\n",
      " -7.0781645e-04  1.4422407e-03]\n",
      "\n",
      "Most similar words to 'language':\n",
      "Language: 0.77\n",
      "new: 0.28\n",
      "learning: 0.27\n",
      "in: 0.16\n",
      "things: 0.08\n",
      "natural: 0.08\n",
      "fun: 0.07\n",
      "I: 0.05\n",
      "processing: 0.04\n",
      "amazing: 0.02\n",
      "\n",
      "Vector for OOV word 'processor':\n",
      "[-2.1812057e-03  2.5991723e-03  3.6927740e-04 -3.3225031e-03\n",
      " -3.3004366e-05 -1.1048903e-03 -1.1847130e-03  3.3407256e-03\n",
      "  1.1815253e-03 -8.7241671e-04 -3.1516471e-04 -9.4935764e-04\n",
      "  5.4635259e-04  5.6649197e-04  2.0462645e-03 -2.9252113e-05\n",
      "  4.4257977e-04 -5.9156987e-04  1.4584794e-03 -4.0844539e-03\n",
      "  9.3460194e-04  1.6634904e-03  6.6962070e-04  1.5514269e-03\n",
      " -3.0466556e-03  3.0620298e-03  1.9693295e-03 -2.0754195e-03\n",
      " -3.7407833e-03 -1.6609818e-03  2.0125738e-04  8.9771929e-04\n",
      " -2.5342568e-03  6.5702415e-04 -6.0982717e-04 -2.1119406e-03\n",
      " -5.7001476e-04 -2.2618861e-03 -2.7735403e-04  1.2677714e-03\n",
      "  8.1143749e-04 -4.4023902e-03  1.1203547e-03 -6.2352564e-04\n",
      " -6.0525647e-04  3.4875516e-04 -4.7129844e-03 -1.9192231e-03\n",
      " -1.1723108e-03  1.5632795e-04]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Sample corpus (list of tokenized sentences)\n",
    "sentences = [\n",
    "    [\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"Language\", \"processing\", \"is\", \"amazing\"],\n",
    "    [\"I\", \"love\", \"learning\", \"new\", \"things\", \"in\", \"NLP\"],\n",
    "    [\"Natural\", \"language\", \"processing\", \"is\", \"fun\"]\n",
    "]\n",
    "\n",
    "# Train FastText model\n",
    "model = FastText(sentences, vector_size=50, window=3, min_count=1, sg=1)  # sg=1 for Skip-gram; sg=0 for CBOW\n",
    "\n",
    "# Example 1: Get the vector for a word\n",
    "print(\"Vector for 'language':\\n\", model.wv['language'])\n",
    "\n",
    "# Example 2: Find similar words\n",
    "similar_words = model.wv.most_similar(\"language\")\n",
    "print(\"\\nMost similar words to 'language':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.2f}\")\n",
    "\n",
    "# Example 3: Handle an out-of-vocabulary (OOV) word\n",
    "oov_word = \"processor\"\n",
    "print(f\"\\nVector for OOV word '{oov_word}':\")\n",
    "print(model.wv[oov_word])  # FastText can generate embeddings for OOV words based on subwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Use Colab with T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4283510776.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Will try something else\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Will try something else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'I love machine learning.': tensor([ 0.0995,  0.2099, -0.1130, -0.2212, -0.4113])...\n",
      "Embedding for 'ELMo embeddings capture syntax and semantics.': tensor([-0.6196, -0.2122, -0.4904, -0.2664, -0.3653])...\n",
      "Embedding for 'Deep learning is awesome!': tensor([ 0.2572,  0.0931,  0.0439,  0.0604, -0.2219])...\n",
      "Embedding for 'Natural language processing is fascinating.': tensor([-0.0247, -0.0657, -0.4309, -0.0512, -0.4532])...\n",
      "Embedding for 'The weather is nice today.': tensor([ 0.2954, -0.0861, -0.1512, -0.2963, -0.1568])...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define sentences for embedding\n",
    "sentences = [\n",
    "    \"I love machine learning.\",\n",
    "    \"ELMo embeddings capture syntax and semantics.\",\n",
    "    \"Deep learning is awesome!\",\n",
    "    \"Natural language processing is fascinating.\",\n",
    "    \"The weather is nice today.\"\n",
    "]\n",
    "\n",
    "# Get embeddings for each sentence\n",
    "with torch.no_grad():\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors='pt')\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        print(f\"Embedding for '{sentence}': {last_hidden_states[0][0][:5]}...\")  # Display first 5 values for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1: 'I love machine learning.'\n",
      "Token 1 Embedding: tensor([-0.0796, -0.0654, -0.0842, -0.0337, -0.0758])...\n",
      "Token 2 Embedding: tensor([ 0.0757, -0.0351, -0.4318,  0.4180, -0.1469])...\n",
      "Token 3 Embedding: tensor([ 0.2298,  0.0530, -0.1179, -0.3026,  0.0565])...\n",
      "Token 4 Embedding: tensor([-0.5382,  0.2217, -1.8012, -0.6799,  0.2488])...\n",
      "Token 5 Embedding: tensor([-0.0386, -0.3294, -0.1851, -0.1470, -0.1241])...\n",
      "Token 6 Embedding: tensor([-0.0819,  0.2561, -0.5680, -0.4453,  0.2434])...\n",
      "\n",
      "Sentence 2: 'GPT models are powerful.'\n",
      "Token 1 Embedding: tensor([-0.2023, -0.1084, -0.1990, -0.0576,  0.0202])...\n",
      "Token 2 Embedding: tensor([ 0.3028, -0.0956,  0.3298,  0.1205,  0.2338])...\n",
      "Token 3 Embedding: tensor([ 0.0383, -0.3765, -0.8445,  0.0795,  0.5644])...\n",
      "Token 4 Embedding: tensor([ 0.0440, -0.0018, -1.0083, -0.1281,  0.3954])...\n",
      "Token 5 Embedding: tensor([ 0.0569, -0.0707, -1.2651, -0.3378,  0.1564])...\n",
      "Token 6 Embedding: tensor([ 0.1741, -0.1055, -0.2135,  0.1332, -0.2807])...\n",
      "\n",
      "Sentence 3: 'Natural language processing is fascinating.'\n",
      "Token 1 Embedding: tensor([-0.0769, -0.0017, -0.4327, -0.0941, -0.0581])...\n",
      "Token 2 Embedding: tensor([-0.4622,  0.0055, -1.4165, -0.0452, -0.5215])...\n",
      "Token 3 Embedding: tensor([-0.2003, -0.1135, -0.8820, -0.3973,  0.0071])...\n",
      "Token 4 Embedding: tensor([-0.3853,  0.2157, -0.4725, -0.2623,  0.0699])...\n",
      "Token 5 Embedding: tensor([-0.3462, -0.0884, -2.2235, -0.3998, -0.0954])...\n",
      "Token 6 Embedding: tensor([ 0.0085, -0.3175, -0.4111, -0.1539, -0.2734])...\n",
      "\n",
      "Sentence 4: 'The weather is nice today.'\n",
      "Token 1 Embedding: tensor([-0.0470, -0.0333, -0.1626, -0.0075, -0.0976])...\n",
      "Token 2 Embedding: tensor([ 0.1544,  0.1633, -0.7621,  0.1802, -0.5391])...\n",
      "Token 3 Embedding: tensor([ 0.2650,  0.5216, -0.6459,  0.0761,  0.4247])...\n",
      "Token 4 Embedding: tensor([-0.1891, -0.3764, -1.0939, -0.1048,  0.5321])...\n",
      "Token 5 Embedding: tensor([ 0.6526, -0.2890, -0.6513,  0.0101, -0.2352])...\n",
      "Token 6 Embedding: tensor([ 0.2820, -0.1874,  0.1893,  0.0831,  0.1079])...\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the padding token to the end-of-sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    \"I love machine learning.\",\n",
    "    \"GPT models are powerful.\",\n",
    "    \"Natural language processing is fascinating.\",\n",
    "    \"The weather is nice today.\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences and convert to tensor format\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Generate embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state  # [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "# Display the embeddings\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"\\nSentence {i+1}: '{sentence}'\")\n",
    "    for j, token_embedding in enumerate(embeddings[i]):\n",
    "        print(f\"Token {j+1} Embedding: {token_embedding[:5]}...\")  # Display first 5 values for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1: 'I love machine learning.'\n",
      "Token 1 Embedding: tensor([ 0.1647, -0.0915,  0.1383, -0.0698,  0.1096])...\n",
      "Token 2 Embedding: tensor([-0.0996,  0.1405, -0.1474, -0.0586,  0.0590])...\n",
      "Token 3 Embedding: tensor([ 0.3002, -0.0310, -0.1677, -0.1467,  0.0316])...\n",
      "Token 4 Embedding: tensor([-0.0468, -0.3225, -0.1392,  0.0620, -0.1065])...\n",
      "Token 5 Embedding: tensor([-0.0811,  0.0449,  0.0255, -0.1082, -0.0470])...\n",
      "Token 6 Embedding: tensor([ 0.1029,  0.0481, -0.0459, -0.0317, -0.0194])...\n",
      "Token 7 Embedding: tensor([ 0.2274,  0.0168,  0.2259, -0.2130, -0.2057])...\n",
      "\n",
      "Sentence 2: 'T5 models are powerful.'\n",
      "Token 1 Embedding: tensor([-0.0280,  0.3353, -0.0237,  0.0870,  0.0312])...\n",
      "Token 2 Embedding: tensor([-0.1778,  0.0678,  0.0381, -0.0259, -0.1440])...\n",
      "Token 3 Embedding: tensor([ 0.1269,  0.0421,  0.0097,  0.0684, -0.0343])...\n",
      "Token 4 Embedding: tensor([-0.0498, -0.1587, -0.2012,  0.0405, -0.0585])...\n",
      "Token 5 Embedding: tensor([-0.1183, -0.1588, -0.0993, -0.2670, -0.1317])...\n",
      "Token 6 Embedding: tensor([-0.0828, -0.0220, -0.0584, -0.0715, -0.1977])...\n",
      "Token 7 Embedding: tensor([ 0.0954,  0.1055, -0.0432, -0.0046, -0.0882])...\n",
      "\n",
      "Sentence 3: 'Natural language processing is fascinating.'\n",
      "Token 1 Embedding: tensor([0.2941, 0.0796, 0.1405, 0.0062, 0.2437])...\n",
      "Token 2 Embedding: tensor([-0.0119, -0.0406,  0.1301,  0.1017, -0.0241])...\n",
      "Token 3 Embedding: tensor([-0.0360, -0.2963,  0.0413,  0.2080,  0.1224])...\n",
      "Token 4 Embedding: tensor([-0.0445,  0.0090, -0.1970,  0.0914, -0.0296])...\n",
      "Token 5 Embedding: tensor([-0.2478, -0.0218, -0.0203, -0.2643, -0.0010])...\n",
      "Token 6 Embedding: tensor([-0.1382, -0.0672, -0.0125, -0.1352, -0.1710])...\n",
      "Token 7 Embedding: tensor([ 0.1122,  0.1209, -0.0376,  0.0244, -0.0487])...\n",
      "\n",
      "Sentence 4: 'The weather is nice today.'\n",
      "Token 1 Embedding: tensor([ 0.3638, -0.1411, -0.0832, -0.3310,  0.0434])...\n",
      "Token 2 Embedding: tensor([ 0.1340, -0.0220,  0.0839, -0.1058,  0.0862])...\n",
      "Token 3 Embedding: tensor([-0.1161,  0.0979, -0.1951, -0.0133,  0.0659])...\n",
      "Token 4 Embedding: tensor([-0.2301,  0.0832, -0.1244, -0.2247, -0.0370])...\n",
      "Token 5 Embedding: tensor([-0.0760, -0.0979, -0.0298,  0.0038, -0.1928])...\n",
      "Token 6 Embedding: tensor([-0.1025, -0.0493, -0.0084, -0.1308, -0.0884])...\n",
      "Token 7 Embedding: tensor([ 0.0703,  0.0847, -0.0202, -0.0083, -0.0205])...\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained T5 encoder model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5EncoderModel.from_pretrained(\"t5-small\")  # Use T5EncoderModel instead of T5Model\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    \"I love machine learning.\",\n",
    "    \"T5 models are powerful.\",\n",
    "    \"Natural language processing is fascinating.\",\n",
    "    \"The weather is nice today.\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences and convert to tensor format\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Generate embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state  # [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "# Display the embeddings\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"\\nSentence {i+1}: '{sentence}'\")\n",
    "    for j, token_embedding in enumerate(embeddings[i]):\n",
    "        print(f\"Token {j+1} Embedding: {token_embedding[:5]}...\")  # Display first 5 values for readability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnipre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
