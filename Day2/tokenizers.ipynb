{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "corpus = \"\"\"Hello world! John works at OpenAI, located in San Francisco. His email is john.doe@example.com. \n",
    "\n",
    "\n",
    "He recently completed a project on tokenization on 12/12/2023, focusing on character-level, word-level, \n",
    "and other advanced techniques. Happiness is the key to success.\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character level tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-Level Tokens (First 50):\n",
      "['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!', ' ', 'J', 'o', 'h', 'n', ' ', 'w', 'o', 'r', 'k', 's', ' ', 'a', 't', ' ', 'O', 'p', 'e', 'n', 'A', 'I', ',', ' ', 'l', 'o', 'c', 'a', 't', 'e', 'd', ' ', 'i', 'n', ' ', 'S', 'a', 'n', ' ']\n",
      "\n",
      "Total Character Tokens: 265\n"
     ]
    }
   ],
   "source": [
    "# Character-level tokenization\n",
    "char_tokens = list(corpus)\n",
    "\n",
    "# Display the first 50 characters as tokens\n",
    "print(\"Character-Level Tokens (First 50):\")\n",
    "print(char_tokens[:50])\n",
    "\n",
    "# Display the total number of character tokens\n",
    "print(\"\\nTotal Character Tokens:\", len(char_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word-Level Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-Level Tokens (First 20):\n",
      "['Hello', 'world', 'John', 'works', 'at', 'OpenAI', 'located', 'in', 'San', 'Francisco', 'His', 'email', 'is', 'john', 'doe', 'example', 'com', 'He', 'recently', 'completed']\n",
      "\n",
      "Total Word Tokens: 44\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Word-level tokenization using regex\n",
    "word_tokens = re.findall(r'\\b\\w+\\b', corpus)\n",
    "\n",
    "# Display the first 20 word tokens\n",
    "print(\"Word-Level Tokens (First 20):\")\n",
    "print(word_tokens[:20])\n",
    "\n",
    "# Display the total number of word tokens\n",
    "print(\"\\nTotal Word Tokens:\", len(word_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence-Level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence-Level Tokens:\n",
      "Sentence 1: Hello world!\n",
      "Sentence 2: John works at OpenAI, located in San Francisco.\n",
      "Sentence 3: His email is john.doe@example.com.\n",
      "Sentence 4: \n",
      "He recently completed a project on tokenization on 12/12/2023, focusing on character-level, word-level, \n",
      "and other advanced techniques.\n",
      "Sentence 5: Happiness is the key to success.\n",
      "\n",
      "Total Sentences: 5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sentence-level tokenization using regular expressions\n",
    "sentence_tokens = re.split(r'(?<=[.!?]) +', corpus)\n",
    "\n",
    "# Display the sentence tokens\n",
    "print(\"Sentence-Level Tokens:\")\n",
    "for idx, sentence in enumerate(sentence_tokens):\n",
    "    print(f\"Sentence {idx + 1}: {sentence}\")\n",
    "\n",
    "# Display the total number of sentences\n",
    "print(\"\\nTotal Sentences:\", len(sentence_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paragraph-Level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph-Level Tokens:\n",
      "Paragraph 1: Hello world! John works at OpenAI, located in San Francisco. His email is john.doe@example.com. \n",
      "He recently completed a project on tokenization on 12/12/2023, focusing on character-level, word-level, \n",
      "and other advanced techniques. Happiness is the key to success.\n",
      "\n",
      "Total Paragraphs: 1\n"
     ]
    }
   ],
   "source": [
    "# Paragraph-level tokenization by splitting on double newlines\n",
    "paragraph_tokens = corpus.split('\\n\\n')\n",
    "\n",
    "# Display the paragraph tokens\n",
    "print(\"Paragraph-Level Tokens:\")\n",
    "for idx, paragraph in enumerate(paragraph_tokens):\n",
    "    print(f\"Paragraph {idx + 1}: {paragraph}\")\n",
    "\n",
    "# Display the total number of paragraphs\n",
    "print(\"\\nTotal Paragraphs:\", len(paragraph_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subword Tokenization\n",
    "\n",
    "1. BPE (Byte Pair Encoding)\n",
    "\n",
    "2. WordPiece\n",
    "\n",
    "3. SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n",
      "Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.8/2.4 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 4.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: fsspec, filelock, huggingface-hub, tokenizers\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.2 tokenizers-0.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte-Pair Encoding (BPE) Tokens:\n",
      "['Hel', 'lo', 'wor', 'l', 'd', '!', 'John', 'wor', 'k', 's', 'at', 'Op', 'en', 'AI', ',', 'lo', 'cated', 'in', 'San', 'Fr', 'an', 'cis', 'co', '.', 'His', 'em', 'ai', 'l', 'is', 'j', 'ohn', '.', 'do', 'e', '@', 'ex', 'ample', '.', 'co', 'm', '.', 'He', 'r', 'e', 'ce', 'n', 't', 'l', 'y', 'co', 'mple', 'ted', 'a', 'p', 'r', 'o', 'j', 'e', 'ct', 'on', 'to', 'k', 'en', 'i', 'z', 'at', 'i', 'on', 'on', '12', '/', '12', '/', '202', '3', ',', 'f', 'o', 'cu', 's', 'in', 'g', 'on', 'ch', 'ar', 'ac', 'te', 'r', '-', 'level', ',', 'wor', 'd', '-', 'level', ',', 'an', 'd', 'o', 'the', 'r', 'ad', 'v', 'an', 'ce', 'd', 'te', 'chn', 'i', 'q', 'u', 'es', '.', 'Ha', 'p', 'p', 'in', 'es', 's', 'is', 'the', 'k', 'e', 'y', 'to', 's', 'u', 'cce', 's', 's', '.']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"\"\"Hello world! John works at OpenAI, located in San Francisco. His email is john.doe@example.com. \n",
    "    He recently completed a project on tokenization on 12/12/2023, focusing on character-level, word-level, \n",
    "    and other advanced techniques. Happiness is the key to success.\"\"\"\n",
    "]\n",
    "\n",
    "# Initialize the BPE tokenizer\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(vocab_size=100, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(corpus, trainer)\n",
    "\n",
    "# Encode the text using the trained BPE tokenizer\n",
    "encoded = tokenizer.encode(corpus[0])\n",
    "\n",
    "# Display BPE tokens\n",
    "print(\"Byte-Pair Encoding (BPE) Tokens:\")\n",
    "print(encoded.tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPiece Tokens:\n",
      "['H', '##el', '##l', '##o', 'wor', '##l', '##d', '!', 'J', '##ohn', 'wor', '##k', '##s', 'a', '##t', 'O', '##p', '##en', '##A', '##I', ',', 'l', '##oc', '##a', '##ted', 'i', '##n', 'S', '##an', 'F', '##r', '##an', '##c', '##is', '##c', '##o', '.', 'H', '##is', 'e', '##m', '##a', '##i', '##l', 'is', 'j', '##ohn', '.', 'd', '##o', '##e', '@', 'e', '##x', '##a', '##mple', '.', 'co', '##m', '.', 'H', '##e', 'r', '##ec', '##en', '##t', '##l', '##y', 'co', '##mple', '##ted', 'a', 'p', '##r', '##o', '##j', '##ec', '##t', 'on', 'to', '##k', '##en', '##i', '##z', '##a', '##t', '##i', '##o', '##n', 'on', '12', '/', '12', '/', '2', '##0', '##2', '##3', ',', 'f', '##oc', '##u', '##s', '##in', '##g', 'on', 'c', '##h', '##a', '##r', '##a', '##c', '##t', '##er', '-', 'le', '##vel', ',', 'wor', '##d', '-', 'le', '##vel', ',', 'a', '##n', '##d', 'o', '##t', '##h', '##er', 'a', '##d', '##v', '##an', '##c', '##ed', 't', '##ec', '##hn', '##i', '##q', '##u', '##es', '.', 'H', '##a', '##p', '##p', '##in', '##es', '##s', 'is', 't', '##h', '##e', 'k', '##e', '##y', 'to', 's', '##u', '##c', '##c', '##es', '##s', '.']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"\"\"Hello world! John works at OpenAI, located in San Francisco. His email is john.doe@example.com. \n",
    "    He recently completed a project on tokenization on 12/12/2023, focusing on character-level, word-level, \n",
    "    and other advanced techniques. Happiness is the key to success.\"\"\"\n",
    "]\n",
    "\n",
    "# Initialize the WordPiece tokenizer\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = WordPieceTrainer(vocab_size=100, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(corpus, trainer)\n",
    "\n",
    "# Encode the text using the trained WordPiece tokenizer\n",
    "encoded = tokenizer.encode(corpus[0])\n",
    "\n",
    "# Display WordPiece tokens\n",
    "print(\"WordPiece Tokens:\")\n",
    "print(encoded.tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 786.4/991.5 kB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 991.5/991.5 kB 5.8 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece Tokens:\n",
      "['▁', 'H', 'e', 'l', 'l', 'o', '▁wor', 'l', 'd', '!', '▁', 'J', 'o', 'h', 'n', '▁wor', 'k', 's', '▁', 'a', 't', '▁', 'O', 'p', 'e', 'n', 'A', 'I', ',', '▁', 'l', 'o', 'c', 'a', 't', 'e', 'd', '▁', 'i', 'n', '▁', 'S', 'a', 'n', '▁', 'F', 'r', 'a', 'n', 'c', 'is', 'co', '.', '▁', 'H', 'is', '▁', 'e', 'm', 'a', 'i', 'l', '▁', 'is', '▁', 'j', 'o', 'h', 'n', '.', 'd', 'o', 'e', '@', 'e', 'x', 'a', 'm', 'p', 'l', 'e', '.', 'co', 'm', '.', '▁', 'H', 'e', '▁', 'r', 'e', 'c', 'e', 'n', 't', 'l', 'y', '▁', 'co', 'm', 'p', 'l', 'e', 't', 'e', 'd', '▁', 'a', '▁', 'p', 'r', 'o', 'j', 'e', 'c', 't', '▁o', 'n', '▁', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', '▁o', 'n', '▁', '1', '2', '/', '1', '2', '/', '2', '0', '2', '3', ',', '▁', 'f', 'o', 'c', 'u', 's', 'i', 'n', 'g', '▁o', 'n', '▁', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', '-', 'l', 'e', 'v', 'e', 'l', ',', '▁wor', 'd', '-', 'l', 'e', 'v', 'e', 'l', ',', '▁', 'a', 'n', 'd', '▁o', 't', 'h', 'e', 'r', '▁', 'a', 'd', 'v', 'a', 'n', 'c', 'e', 'd', '▁', 't', 'e', 'c', 'h', 'n', 'i', 'q', 'u', 'e', 's', '.', '▁', 'H', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's', '▁', 'is', '▁', 't', 'h', 'e', '▁', 'k', 'e', 'y', '▁', 't', 'o', '▁', 's', 'u', 'c', 'c', 'e', 's', 's', '.']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "# Save the corpus to a temporary file (SentencePiece requires input from a file)\n",
    "with open(\"temp_corpus.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"Hello world! John works at OpenAI, located in San Francisco. His email is john.doe@example.com. \n",
    "    He recently completed a project on tokenization on 12/12/2023, focusing on character-level, word-level, \n",
    "    and other advanced techniques. Happiness is the key to success.\"\"\")\n",
    "\n",
    "# Train the SentencePiece model with a reduced vocabulary size\n",
    "spm.SentencePieceTrainer.Train('--input=temp_corpus.txt --model_prefix=sp_model --vocab_size=50')\n",
    "\n",
    "# Load the trained model\n",
    "sp = spm.SentencePieceProcessor(model_file='sp_model.model')\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokens = sp.encode(\"\"\"Hello world! John works at OpenAI, located in San Francisco. His email is john.doe@example.com. \n",
    "He recently completed a project on tokenization on 12/12/2023, focusing on character-level, word-level, \n",
    "and other advanced techniques. Happiness is the key to success.\"\"\", out_type=str)\n",
    "\n",
    "# Display SentencePiece tokens\n",
    "print(\"SentencePiece Tokens:\")\n",
    "print(tokens)\n",
    "\n",
    "# Cleanup temporary files\n",
    "os.remove(\"temp_corpus.txt\")\n",
    "os.remove(\"sp_model.model\")\n",
    "os.remove(\"sp_model.vocab\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphological Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (0.13.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (4.67.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\basudev\\genaiprereq\\day1\\gnipre\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morphological Tokens:\n",
      "Word: He, Lemma: he, Morphology: Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\n",
      "Word: recently, Lemma: recently, Morphology: \n",
      "Word: completed, Lemma: complete, Morphology: Tense=Past|VerbForm=Fin\n",
      "Word: a, Lemma: a, Morphology: Definite=Ind|PronType=Art\n",
      "Word: project, Lemma: project, Morphology: Number=Sing\n",
      "Word: on, Lemma: on, Morphology: \n",
      "Word: tokenization, Lemma: tokenization, Morphology: Number=Sing\n",
      "Word: ,, Lemma: ,, Morphology: PunctType=Comm\n",
      "Word: focusing, Lemma: focus, Morphology: Aspect=Prog|Tense=Pres|VerbForm=Part\n",
      "Word: on, Lemma: on, Morphology: \n",
      "Word: character, Lemma: character, Morphology: Number=Sing\n",
      "Word: -, Lemma: -, Morphology: PunctType=Dash\n",
      "Word: level, Lemma: level, Morphology: Number=Sing\n",
      "Word: ,, Lemma: ,, Morphology: PunctType=Comm\n",
      "Word: word, Lemma: word, Morphology: Number=Sing\n",
      "Word: -, Lemma: -, Morphology: PunctType=Dash\n",
      "Word: level, Lemma: level, Morphology: Number=Sing\n",
      "Word: ,, Lemma: ,, Morphology: PunctType=Comm\n",
      "Word: and, Lemma: and, Morphology: ConjType=Cmp\n",
      "Word: morphological, Lemma: morphological, Morphology: Degree=Pos\n",
      "Word: analysis, Lemma: analysis, Morphology: Number=Sing\n",
      "Word: ., Lemma: ., Morphology: PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"He recently completed a project on tokenization, focusing on character-level, word-level, and morphological analysis.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract morphemes using spaCy's token analysis\n",
    "print(\"Morphological Tokens:\")\n",
    "for token in doc:\n",
    "    # Display the token, its lemma, and morphological details\n",
    "    print(f\"Word: {token.text}, Lemma: {token.lemma_}, Morphology: {token.morph}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Gram Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams (n=2):\n",
      "('He', 'recently')\n",
      "('recently', 'completed')\n",
      "('completed', 'a')\n",
      "('a', 'project')\n",
      "('project', 'on')\n",
      "('on', 'tokenization,')\n",
      "('tokenization,', 'focusing')\n",
      "('focusing', 'on')\n",
      "('on', 'character-level,')\n",
      "('character-level,', 'word-level,')\n",
      "('word-level,', 'and')\n",
      "('and', 'morphological')\n",
      "('morphological', 'analysis.')\n",
      "\n",
      "Trigrams (n=3):\n",
      "('He', 'recently', 'completed')\n",
      "('recently', 'completed', 'a')\n",
      "('completed', 'a', 'project')\n",
      "('a', 'project', 'on')\n",
      "('project', 'on', 'tokenization,')\n",
      "('on', 'tokenization,', 'focusing')\n",
      "('tokenization,', 'focusing', 'on')\n",
      "('focusing', 'on', 'character-level,')\n",
      "('on', 'character-level,', 'word-level,')\n",
      "('character-level,', 'word-level,', 'and')\n",
      "('word-level,', 'and', 'morphological')\n",
      "('and', 'morphological', 'analysis.')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "# Sample text\n",
    "text = \"He recently completed a project on tokenization, focusing on character-level, word-level, and morphological analysis.\"\n",
    "\n",
    "# Split the text into words\n",
    "words = text.split()\n",
    "\n",
    "# Generate bigrams (n=2)\n",
    "bigrams = list(ngrams(words, 2))\n",
    "print(\"Bigrams (n=2):\")\n",
    "for bigram in bigrams:\n",
    "    print(bigram)\n",
    "\n",
    "# Generate trigrams (n=3)\n",
    "trigrams = list(ngrams(words, 3))\n",
    "print(\"\\nTrigrams (n=3):\")\n",
    "for trigram in trigrams:\n",
    "    print(trigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syllable-Level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting syllapy\n",
      "  Downloading syllapy-0.7.2-py3-none-any.whl.metadata (854 bytes)\n",
      "Downloading syllapy-0.7.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: syllapy\n",
      "Successfully installed syllapy-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install syllapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syllable Count for Each Word:\n",
      "Word: He, Syllable Count: 1\n",
      "Word: recently, Syllable Count: 2\n",
      "Word: completed, Syllable Count: 3\n",
      "Word: a, Syllable Count: 1\n",
      "Word: project, Syllable Count: 2\n",
      "Word: on, Syllable Count: 1\n",
      "Word: tokenization,, Syllable Count: 5\n",
      "Word: focusing, Syllable Count: 3\n",
      "Word: on, Syllable Count: 1\n",
      "Word: character-level,, Syllable Count: 5\n",
      "Word: word-level,, Syllable Count: 3\n",
      "Word: and, Syllable Count: 1\n",
      "Word: morphological, Syllable Count: 5\n",
      "Word: analysis., Syllable Count: 4\n"
     ]
    }
   ],
   "source": [
    "import syllapy\n",
    "\n",
    "# Sample text\n",
    "text = \"He recently completed a project on tokenization, focusing on character-level, word-level, and morphological analysis.\"\n",
    "\n",
    "# Tokenize by words first\n",
    "words = text.split()\n",
    "\n",
    "# Perform syllable-level counting\n",
    "print(\"Syllable Count for Each Word:\")\n",
    "for word in words:\n",
    "    # Get the syllable count for each word\n",
    "    syllable_count = syllapy.count(word)\n",
    "    print(f\"Word: {word}, Syllable Count: {syllable_count}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Rules for Counting Syllables\n",
    "Here are some basic rules to estimate syllable counts:\n",
    "\n",
    "Each Vowel Sound is Usually a Syllable:\n",
    "\n",
    "Examples: \"Cat\" (1 syllable), \"Elephant\" (3 syllables: e-le-phant)\n",
    "Consonant-Vowel Patterns:\n",
    "\n",
    "When consonants are placed between vowels, they often divide syllables.\n",
    "Example: \"Tokenization\" (5 syllables: to-ken-i-za-tion).\n",
    "Silent Letters:\n",
    "\n",
    "Silent vowels don't add a syllable.\n",
    "Example: \"Bake\" (1 syllable), where the \"e\" is silent.\n",
    "Prefixes and Suffixes:\n",
    "\n",
    "Prefixes and suffixes often add syllables if they have vowel sounds.\n",
    "Example: \"Happiness\" (3 syllables: hap-pi-ness).\n",
    "Examples of Syllable Counts in Words from the Sentence\n",
    "Let's break down the syllable counts in some words from your example sentence:\n",
    "\n",
    "\"He\": 1 syllable (one vowel sound)\n",
    "\"Recently\": 3 syllables (re-cen-tly)\n",
    "\"Re\" and \"cent\" both contain vowel sounds separated by consonants.\n",
    "\"Completed\": 3 syllables (com-ple-ted)\n",
    "\"Com,\" \"ple,\" and \"ted\" each contain distinct vowel sounds.\n",
    "\"Tokenization\": 5 syllables (to-ken-i-za-tion)\n",
    "This word contains multiple vowel sounds, creating five syllables.\n",
    "Why Syllable Counts Matter\n",
    "Syllable counts are often used in:\n",
    "\n",
    "Poetry and Rhythm: Where the number of syllables affects the rhythm of a verse.\n",
    "Speech and Pronunciation: Breaking words into syllables helps with proper pronunciation.\n",
    "Speech Processing: Syllable-level tokenization can be useful in applications like text-to-speech and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity-Level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity-Level Tokens:\n",
      "Entity: r. John Doe, Label: PERSON\n",
      "Entity: Google, Label: ORG\n",
      "Entity: New York, Label: GPE\n",
      "Entity: September 15, 2023, Label: DATE\n",
      "Entity: the Tech Conference, Label: FAC\n",
      "Entity: Microsoft, Label: ORG\n",
      "Entity: Seattle, Label: GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"r. John Doe, a researcher at Google, traveled to New York on September 15, 2023, to attend the Tech Conference. \n",
    "You can contact him at john.doe@google.com for more information about the project. He previously worked for Microsoft in Seattle,\"\"\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities\n",
    "print(\"Entity-Level Tokens:\")\n",
    "for entity in doc.ents:\n",
    "    print(f\"Entity: {entity.text}, Label: {entity.label_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex-Based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails Found:\n",
      "john.doe@example.com\n",
      "\n",
      "Dates Found:\n",
      "09/30/2023\n",
      "September\n",
      "\n",
      "URLs Found:\n",
      "https://www.example.com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Dr. John Doe can be reached at john.doe@example.com. \n",
    "The project deadline is 09/30/2023, and the report is due on September 15, 2023. \n",
    "Visit our website at https://www.example.com for more information.\n",
    "\"\"\"\n",
    "\n",
    "# Regex patterns for various entities\n",
    "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "date_pattern = r'\\b(\\d{2}/\\d{2}/\\d{4})\\b|\\b(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\\b'\n",
    "url_pattern = r'https?://(?:www\\.)?[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?:/[a-zA-Z0-9._%+-]*)*'\n",
    "\n",
    "\n",
    "# Extract emails\n",
    "emails = re.findall(email_pattern, text)\n",
    "print(\"Emails Found:\")\n",
    "for email in emails:\n",
    "    print(email)\n",
    "\n",
    "# Extract dates\n",
    "dates = re.findall(date_pattern, text)\n",
    "print(\"\\nDates Found:\")\n",
    "for date in dates:\n",
    "    # date is a tuple with two parts; we only want the non-empty part\n",
    "    print(\"\".join(date))\n",
    "\n",
    "# Extract URLs\n",
    "urls = re.findall(url_pattern, text)\n",
    "print(\"\\nURLs Found:\")\n",
    "for url in urls:\n",
    "    print(url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnipre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
